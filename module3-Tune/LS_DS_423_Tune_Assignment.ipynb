{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "\n",
    "\n",
    "# Tune Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
    "\n",
    "# Gridsearch Hyperparameters\n",
    "\n",
    "In the guided project, you learned how to use sklearn's GridsearchCV and keras-tuner library to tune the hyperparamters of a neural network model. For your module project you'll continue using these two libraries however we are going to make things a little more interesting for you. \n",
    "\n",
    "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
    "\n",
    "\n",
    "\n",
    "**Don't forgot to switch to GPU on Colab!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# native python libraries imports \n",
    "import math\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# sklearn imports \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# keras imports \n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import get_file\n",
    "\n",
    "# required for compatibility between sklearn and keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quickdraw10():\n",
    "    \"\"\"\n",
    "    Fill out this doc string, and comment the code, for practice in writing the kind of code that will get you hired. \n",
    "    \"\"\"\n",
    "    \n",
    "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
    "    \n",
    "    path_to_zip = get_file('../quickdraw10.npz', origin=URL_, extract=False)\n",
    "\n",
    "    data = np.load(path_to_zip)\n",
    "    \n",
    "    # normalize your image data\n",
    "    max_pixel_value = 255\n",
    "    X = data['arr_0']/max_pixel_value\n",
    "    Y = data['arr_1']\n",
    "        \n",
    "    return train_test_split(X, Y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_quickdraw10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# Experiment 1\n",
    "\n",
    "## Tune Hyperperameters using Enhanced GridsearchCV \n",
    "\n",
    "We are going to use GridsearchCV again to tune a deep learning model however we are going to add some additional functionality to our gridsearch. Specifically, we are going to automate away the generation of how many nodes to use in a layer and how many layers to use in a model! \n",
    "\n",
    "By the way, yes, there is a function within a function. Try to not let that bother you. An alternative to this would be to create a class. If you're up for the challenge give it a shot. However, consider this a stretch goal that you come back to after you finish going through this assignment. \n",
    "\n",
    "\n",
    "### Objective \n",
    "\n",
    "The objective of this experiment is to show you how to automate the generation of layers and layer nodes for the purposes of gridsearch. Up until now, we've been manually selecting the number of layers and layer nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USXjs7Hk71Hy"
   },
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
    "    \"\"\"\"\n",
    "    Returns a complied keras model \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_layers: int \n",
    "        number of hidden layers in model \n",
    "        To be clear, this excludes the input and output layer.\n",
    "        \n",
    "    first_layer_nodes: int\n",
    "        Number of nodes in the first hidden layer \n",
    "\n",
    "    last_layer_nodes: int\n",
    "        Number of nodes in the last hidden layer (this is the layer just prior to the output layer)\n",
    "        \n",
    "     act_funct: string \n",
    "         Name of activation function to use in hidden layers (this excludes the output layler)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model: keras object \n",
    "    \"\"\"\n",
    "    \n",
    "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
    "        \"\"\"\n",
    "        Generates and returns the number of nodes in each hidden layer. \n",
    "        To be clear, this excludes the input and output layer. \n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        Number of nodes in each layer is linearly incremented. \n",
    "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_layers: int\n",
    "            Number of hidden layers\n",
    "            This values should be 2 or greater \n",
    "\n",
    "        first_layer_nodes: int\n",
    "\n",
    "        last_layer_nodes: int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        layers: list of ints\n",
    "            Contains number of nodes for each layer \n",
    "        \"\"\"\n",
    "\n",
    "        # throws an error if n_layers is less than 2 \n",
    "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
    "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
    "        # when set to True number of nodes are decreased for subsequent layers \n",
    "        if negative_node_incrementation:\n",
    "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
    "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
    "            \n",
    "        # when set to False number of nodes are increased for subsequent layers\n",
    "        else:\n",
    "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
    "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
    "\n",
    "        nodes = first_layer_nodes\n",
    "\n",
    "        for i in range(1, n_layers+1):\n",
    "\n",
    "            layers.append(math.ceil(nodes))\n",
    "\n",
    "            # increment nodes for next layer \n",
    "            nodes = nodes + nodes_increment\n",
    "\n",
    "        return layers\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
    "    \n",
    "    for i in range(1, n_layers):\n",
    "        if i==1:\n",
    "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
    "        else:\n",
    "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
    "            \n",
    "            \n",
    "    # output layer \n",
    "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
    "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer='adam', # adam is a good default optimizer \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # do not include model.fit() inside the create_model function\n",
    "    # KerasClassifier is expecting a complied model \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore create_model\n",
    "\n",
    "Let's build a few different models in order to understand how the above code works in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dcf5c585f07629a03086cf57ba53615",
     "grade": false,
     "grade_id": "cell-86d63e89a21223de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model \n",
    "\n",
    "Use `create_model` to build a model. \n",
    "\n",
    "- Set `n_layers = 10` \n",
    "- Set `first_layer_nodes = 500`\n",
    "- Set `last_layer_nodes = 100`\n",
    "- Set `act_funct = \"relu\"`\n",
    "- Make sure that `negative_node_incrementation = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0722533c325d699f4842e874e43720e",
     "grade": false,
     "grade_id": "cell-99d563a291231a7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use create_model to create a model \n",
    "\n",
    "model = create_model(n_layers=10, first_layer_nodes=500,\n",
    "                     last_layer_nodes=100, act_funct=\"relu\",\n",
    "                     negative_node_incrementation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 545)               273045    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 589)               321594    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 634)               374060    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 678)               430530    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 723)               490917    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 767)               555308    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 812)               623616    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 856)               695928    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                8570      \n",
      "=================================================================\n",
      "Total params: 4,166,068\n",
      "Trainable params: 4,166,068\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# run model.summary() and make sure that you understand the model architecture that you just built \n",
    "# Notice in the model summary how the number of nodes have been linearly incremented in increasing values.\n",
    "# The output layer must have 10 nodes because there are 10 labels to predict \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2344/2344 [==============================] - 156s 66ms/step - loss: 1.0409 - accuracy: 0.6523\n"
     ]
    }
   ],
   "source": [
    "# feel free to play around with parameters to gain additional insight as to how the create_model function works \n",
    "\n",
    "mod_fit = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we've played around a bit with  `create_model` in order to understand how it works, let's build a much simpler model that we'll be running gridsearches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model \n",
    "\n",
    "Use `create_model` to build a model. \n",
    "\n",
    "- Set `n_layers = 2` \n",
    "- Set `first_layer_nodes = 500`\n",
    "- Set `last_layer_nodes = 100`\n",
    "- Set `act_funct = \"relu\"`\n",
    "- Make sure that `negative_node_incrementation = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "606b85d0ba4531836f97caf6850297f8",
     "grade": false,
     "grade_id": "cell-4ca6c5e51302fd10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use create_model to create a model \n",
    "\n",
    "model2 = create_model(n_layers=2, first_layer_nodes=500,\n",
    "                     last_layer_nodes=100, act_funct=\"relu\",\n",
    "                     negative_node_incrementation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 397,510\n",
      "Trainable params: 397,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# run model.summary() and make sure that you understand the model architecture that you just built \n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {'n_layers': [2, 3],\n",
    "              'epochs': [3, 5], \n",
    "              \"first_layer_nodes\": [500, 450],\n",
    "              \"last_layer_nodes\": [100, 150]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = KerasClassifier(create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Epoch 1/5\n",
      "2344/2344 [==============================] - 35s 14ms/step - loss: 0.7281 - accuracy: 0.7761\n",
      "Epoch 2/5\n",
      "2344/2344 [==============================] - 32s 13ms/step - loss: 0.4104 - accuracy: 0.8762\n",
      "Epoch 3/5\n",
      "2344/2344 [==============================] - 30s 13ms/step - loss: 0.3238 - accuracy: 0.9008\n",
      "Epoch 4/5\n",
      "2344/2344 [==============================] - 29s 12ms/step - loss: 0.2605 - accuracy: 0.9178\n",
      "Epoch 5/5\n",
      "2344/2344 [==============================] - 30s 13ms/step - loss: 0.2097 - accuracy: 0.9331\n",
      "Best: 0.8709866801897684 using {'epochs': 5, 'first_layer_nodes': 500, 'last_layer_nodes': 150, 'n_layers': 3}\n",
      "Means: 0.8646666606267294, Stdev: 0.0013749365169825 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
      "Means: 0.8642933170000712, Stdev: 0.00279243280669876 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
      "Means: 0.866320013999939, Stdev: 0.000735391181088688 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 150, 'n_layers': 2}\n",
      "Means: 0.8646399776140848, Stdev: 0.002384842595554475 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 150, 'n_layers': 3}\n",
      "Means: 0.8627466758092245, Stdev: 0.001592427791065378 with: {'epochs': 3, 'first_layer_nodes': 450, 'last_layer_nodes': 100, 'n_layers': 2}\n",
      "Means: 0.8644533356030782, Stdev: 0.002290011721484684 with: {'epochs': 3, 'first_layer_nodes': 450, 'last_layer_nodes': 100, 'n_layers': 3}\n",
      "Means: 0.861519992351532, Stdev: 0.003867541570223837 with: {'epochs': 3, 'first_layer_nodes': 450, 'last_layer_nodes': 150, 'n_layers': 2}\n",
      "Means: 0.8667866587638855, Stdev: 0.0019899440014772758 with: {'epochs': 3, 'first_layer_nodes': 450, 'last_layer_nodes': 150, 'n_layers': 3}\n",
      "Means: 0.8678399920463562, Stdev: 0.0010187679496036055 with: {'epochs': 5, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
      "Means: 0.8651866714159647, Stdev: 0.0006344106494815975 with: {'epochs': 5, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
      "Means: 0.8700000047683716, Stdev: 0.0004393825108469368 with: {'epochs': 5, 'first_layer_nodes': 500, 'last_layer_nodes': 150, 'n_layers': 2}\n",
      "Means: 0.8709866801897684, Stdev: 0.0016737538659869712 with: {'epochs': 5, 'first_layer_nodes': 500, 'last_layer_nodes': 150, 'n_layers': 3}\n",
      "Means: 0.8682133356730143, Stdev: 0.0012859565972776134 with: {'epochs': 5, 'first_layer_nodes': 450, 'last_layer_nodes': 100, 'n_layers': 2}\n",
      "Means: 0.8676133354504904, Stdev: 0.0030467116157805 with: {'epochs': 5, 'first_layer_nodes': 450, 'last_layer_nodes': 100, 'n_layers': 3}\n",
      "Means: 0.8671199878056844, Stdev: 0.0018042977624314518 with: {'epochs': 5, 'first_layer_nodes': 450, 'last_layer_nodes': 150, 'n_layers': 2}\n",
      "Means: 0.8656266729036967, Stdev: 0.0015934171677535833 with: {'epochs': 5, 'first_layer_nodes': 450, 'last_layer_nodes': 150, 'n_layers': 3}\n"
     ]
    }
   ],
   "source": [
    "start1 = time()\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model2, \n",
    "                    param_grid=param_grid, \n",
    "                    n_jobs=-2, \n",
    "                    verbose=1, \n",
    "                    cv=3)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "end1 = time()\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.975959380467735"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total run time \n",
    "total_run_time_in_miniutes = (end1 - start1)/60\n",
    "total_run_time_in_miniutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 5,\n",
       " 'first_layer_nodes': 500,\n",
       " 'last_layer_nodes': 150,\n",
       " 'n_layers': 3,\n",
       " 'build_fn': <function __main__.create_model(n_layers, first_layer_nodes, last_layer_nodes, act_funct='relu', negative_node_incrementation=True)>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Experiment 2\n",
    "\n",
    "## Benchmark different Optimization Algorithms \n",
    "\n",
    "In this section, we are going to use the same model and dataset in order to benchmark 3 different gridsearch approaches: \n",
    "\n",
    "- Random Search\n",
    "- Bayesian Optimization. \n",
    "- Brute Force Gridsearch\n",
    "\n",
    "Our goal in this experiment is two-fold. We want to see which appraoch \n",
    "\n",
    "- Scores the highest accuracy\n",
    "- Has the shortest run time \n",
    "\n",
    "We want to see how these 3 gridsearch approaches handle these trade-offs and to give you a sense of those trades offs.\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "`Brute Force Gridsearch` will train a model on every single unique hyperparameter combination, this guarantees that you'll get the highest possible accuracy from your parameter set but your gridsearch might have a very long run-time. \n",
    "\n",
    "`Random Search` will randomly sample from your parameter set which, depending on how many samples, the run-time might be significantly cut down but you might or might not sample the parameters that correspond to the heightest possible accuracies. \n",
    "\n",
    "`Bayesian Optimization` has a bit of intelligence built into it's search algorithm but you do need to manually select some parameters which greatly influence the model learning outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because gridsearching can take a lot of time and we are bench marking 3 different approaches\n",
    "# let's build a simple model to minimize run time \n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # hidden layer\n",
    "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build out our hyperparameter dictionary \n",
    "# hp = HyperParameters()\n",
    "# hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "# hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
    "# hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Run the Gridsearch Algorithms \n",
    "\n",
    "### Random Search\n",
    "\n",
    "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaff9aae33845f374e15f2381719d83a",
     "grade": false,
     "grade_id": "cell-8c1dfb9b6d12bea2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# how many unique hyperparameter combinations do we have? \n",
    "# HINT: take the product of the number of possible values for each hyperparameter \n",
    "# save your answer to n_unique_hparam_combos\n",
    "\n",
    "n_unique_hparam_combos = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9d628451e83431e1b52da10eccf2c00",
     "grade": false,
     "grade_id": "cell-1fa83950bb2d5f92",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# how many of these do we want to randomly sample?\n",
    "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
    "# save this number to n_param_combos_to_sample\n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # hidden layer\n",
    "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build out our hyperparameter dictionary \n",
    "hp = HyperParameters()\n",
    "hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "hp.Choice('learning_rate',values=[1e-2, 1e-3])\n",
    "hp.Choice('activation',values=[\"relu\", \"sigmoid\", \"softmax\"])\n",
    "\n",
    "n_param_combos_to_sample = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tuner = RandomSearch(\n",
    "            build_model,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
    "            seed=1234,\n",
    "            hyperparameters=hp,\n",
    "            directory='./keras-tuner-trial',\n",
    "            project_name='random_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24 Complete [00h 01m 04s]\n",
      "val_accuracy: 0.8138399720191956\n",
      "\n",
      "Best val_accuracy So Far: 0.8733199834823608\n",
      "Total elapsed time: 00h 25m 24s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# take note of Total elapsed time in print out\n",
    "random_tuner.search(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./keras-tuner-trial/random_search\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8733199834823608\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 352\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8695200085639954\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 320\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8693600296974182\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 192\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8688399791717529\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 448\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8627600073814392\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 320\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8616799712181091\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 96\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8575199842453003\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 192\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8548799753189087\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 160\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8501999974250793\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8406400084495544\n"
     ]
    }
   ],
   "source": [
    "# identify the best score and hyperparamter (should be at the top since scores are ranked)\n",
    "random_tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Results\n",
    " \n",
    "Identify and write the the best performing hyperparamter combination and model score. \n",
    "Note that because this is Random Search, multiple runs might have slighly different outcomes. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f084b5d373f8589a1de8d6d4473b974a",
     "grade": true,
     "grade_id": "cell-5527738b6382c164",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "`\"relu\"` activation; 384 units; learning rate 0.001 = 87.33% (total time= 25.4 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Bayesian Optimization\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
    "\n",
    "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
    "\n",
    "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
    "\n",
    "`num_initial_points`: \n",
    "\n",
    "Number of randomly selected hyperparameter combinations to try before applying bayesian probability to determine liklihood of which param combo to try next based on expected improvement\n",
    "\n",
    "\n",
    "`beta`: \n",
    "\n",
    "Larger values means more willing to explore new hyperparameter combinations (analogous to searching for the global minimum in Gradient Descent), smaller values means that it is less willing to try new hyperparameter combinations (analogous to getting stuck in a local minimum in Gradient Descent). \n",
    "\n",
    "As a start, error on the side of larger values. What defines a small or large value you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
    "# because BO isn't random (after num_initial_points number of trails) let's see if 15 max trials gives good results\n",
    "# feel free to play with any of these numbers\n",
    "max_trials=20\n",
    "num_initial_points=5\n",
    "beta=3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_tuner = BayesianOptimization(\n",
    "                    build_model,\n",
    "                    objective='val_accuracy',\n",
    "                    max_trials=max_trials,\n",
    "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
    "                    num_initial_points=num_initial_points, \n",
    "                    beta=beta, \n",
    "                    seed=1234,\n",
    "                    directory='./keras-tuner-trial',\n",
    "                    project_name='bayesian_optimization_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 01m 17s]\n",
      "val_accuracy: 0.8716400265693665\n",
      "\n",
      "Best val_accuracy So Far: 0.877560019493103\n",
      "Total elapsed time: 00h 23m 41s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner.search(X_train, y_train,\n",
    "               epochs=3,\n",
    "               validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.877560019493103\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.875760018825531\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 480\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8753200173377991\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8750399947166443\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 480\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8743600249290466\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 448\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8737999796867371\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8734800219535828\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8721200227737427\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8719599843025208\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 448\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8716400265693665\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Results\n",
    " \n",
    "Identify and write the the best performing hyperparamter combination and model score. \n",
    "Note that because this is  Bayesian Optimization, multiple runs might have slighly different outcomes. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
     "grade": true,
     "grade_id": "cell-ff95600bf745f40f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## Brute Force Gridsearch Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate a Sklearn compatiable parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build out our hyperparameter dictionary \n",
    "hyper_parameters = {\n",
    "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
    "    \"units\": np.arange(32, 544, 32).tolist(),\n",
    "    \"learning_rate\": [1e-2, 1e-3],\n",
    "    \"activation\":[\"relu\", \"sigmoid\", \"softmax\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'units': [32,\n",
       "  64,\n",
       "  96,\n",
       "  128,\n",
       "  160,\n",
       "  192,\n",
       "  224,\n",
       "  256,\n",
       "  288,\n",
       "  320,\n",
       "  352,\n",
       "  384,\n",
       "  416,\n",
       "  448,\n",
       "  480,\n",
       "  512],\n",
       " 'learning_rate': [0.01, 0.001],\n",
       " 'activation': ['relu', 'sigmoid', 'softmax']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Sklearn compatiable model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(units, learning_rate, activation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a complie keras model ready for keras-tuner gridsearch algorithms \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # hidden layer\n",
    "    model.add(Dense(units, activation=activation))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn = build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "2344/2344 [==============================] - 48s 12ms/step - loss: 0.7613 - accuracy: 0.7674\n",
      "Best: 0.8414266705513 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
      "Means: 0.7887066602706909, Stdev: 0.0067501037916762 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
      "Means: 0.7960000038146973, Stdev: 0.005406970989249018 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
      "Means: 0.8009600043296814, Stdev: 0.005661484329403146 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
      "Means: 0.800706684589386, Stdev: 0.0013423760011555441 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
      "Means: 0.8077733318010966, Stdev: 0.0006216973459672794 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
      "Means: 0.801746686299642, Stdev: 0.0026038570702125756 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
      "Means: 0.8008933464686075, Stdev: 0.0016820102281485008 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
      "Means: 0.798146665096283, Stdev: 0.00237010985274301 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
      "Means: 0.8053599993387858, Stdev: 0.00374223227704999 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
      "Means: 0.7996933460235596, Stdev: 0.006111507461274998 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
      "Means: 0.7987866600354513, Stdev: 0.005421682331979009 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
      "Means: 0.7881066600481669, Stdev: 0.007781295246115966 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
      "Means: 0.7961333394050598, Stdev: 0.0032914176901633952 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
      "Means: 0.7995466589927673, Stdev: 0.010927374637122986 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
      "Means: 0.804253339767456, Stdev: 0.0050259312255634 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
      "Means: 0.8000933329264323, Stdev: 0.004895310056218301 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 512}\n",
      "Means: 0.7875600059827169, Stdev: 0.007087205804173115 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
      "Means: 0.8120799859364828, Stdev: 0.0038609209258973227 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
      "Means: 0.8201199769973755, Stdev: 0.0026268295657209126 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
      "Means: 0.8245733380317688, Stdev: 0.004573366758668765 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
      "Means: 0.8285999894142151, Stdev: 0.002720204020626744 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
      "Means: 0.8311466574668884, Stdev: 0.004698292322521253 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
      "Means: 0.8362666567166647, Stdev: 0.0030411249998270377 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
      "Means: 0.8361733357111613, Stdev: 0.00037852927976712473 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
      "Means: 0.8410666783650717, Stdev: 0.0029394492460881223 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
      "Means: 0.8406533400217692, Stdev: 0.0029634761784934354 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
      "Means: 0.8384133179982504, Stdev: 0.0022238299588354113 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
      "Means: 0.839959998925527, Stdev: 0.000979788149429398 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
      "Means: 0.8386933406194051, Stdev: 0.0040666871028914725 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
      "Means: 0.8412800033887228, Stdev: 0.0007222246466256398 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
      "Means: 0.8387866417566935, Stdev: 0.0027345338623567853 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
      "Means: 0.8414266705513, Stdev: 0.0027415548600672715 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
      "Means: 0.7913733323415121, Stdev: 0.003750089928476619 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
      "Means: 0.8113866448402405, Stdev: 0.0051800963604870095 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
      "Means: 0.80349334081014, Stdev: 0.0032596765462800186 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
      "Means: 0.8128533363342285, Stdev: 0.002579981646787885 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
      "Means: 0.8187733292579651, Stdev: 0.001913680059081807 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
      "Means: 0.8214266498883566, Stdev: 0.003108227752907222 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
      "Means: 0.8170933524767557, Stdev: 0.002091841187569058 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
      "Means: 0.8206400076548258, Stdev: 0.0022777884554322373 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
      "Means: 0.8195199966430664, Stdev: 0.0074417967899776455 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
      "Means: 0.8163066506385803, Stdev: 0.0021108796410102596 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
      "Means: 0.8220799763997396, Stdev: 0.0049225501131540395 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
      "Means: 0.821506679058075, Stdev: 0.002458262499057942 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
      "Means: 0.8225066661834717, Stdev: 0.0025332700873213396 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
      "Means: 0.8218800028165182, Stdev: 0.004896488925869276 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
      "Means: 0.815666655699412, Stdev: 0.006546436654002832 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
      "Means: 0.8141599893569946, Stdev: 0.0024273974233929058 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 512}\n",
      "Means: 0.7611599961916605, Stdev: 0.0012903883359740931 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
      "Means: 0.7769733270009359, Stdev: 0.002424398297807164 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
      "Means: 0.78438667456309, Stdev: 0.0033645099335672113 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
      "Means: 0.7894266843795776, Stdev: 0.004027405484049668 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
      "Means: 0.793613334496816, Stdev: 0.0010989869440112794 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
      "Means: 0.793293317159017, Stdev: 0.0031207423610496543 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
      "Means: 0.7931333184242249, Stdev: 0.004462861281321622 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
      "Means: 0.7931333382924398, Stdev: 0.002712417834655224 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
      "Means: 0.7999599973360697, Stdev: 0.0014754774010566098 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
      "Means: 0.8014799952507019, Stdev: 0.005240150285589627 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
      "Means: 0.7957200010617574, Stdev: 0.0046652032833358285 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
      "Means: 0.8041466474533081, Stdev: 0.003643050109435977 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
      "Means: 0.8005466461181641, Stdev: 0.0006165063993162167 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
      "Means: 0.799893319606781, Stdev: 0.00462622968651289 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
      "Means: 0.7997333407402039, Stdev: 0.008192779367069258 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n",
      "Means: 0.7963599960009257, Stdev: 0.007740593042721549 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 512}\n",
      "Means: 0.6922000050544739, Stdev: 0.016057646065034965 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 32}\n",
      "Means: 0.7247733275095621, Stdev: 0.04164990627040548 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 64}\n",
      "Means: 0.7447066704432169, Stdev: 0.024919544146671864 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 96}\n",
      "Means: 0.7415733337402344, Stdev: 0.02059387879825522 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 128}\n",
      "Means: 0.7031466563542684, Stdev: 0.01979556029116024 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 160}\n",
      "Means: 0.75, Stdev: 0.008272757802207391 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 192}\n",
      "Means: 0.7080400188763937, Stdev: 0.015800483974594347 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 224}\n",
      "Means: 0.7262400190035502, Stdev: 0.025605765901828826 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 256}\n",
      "Means: 0.723360002040863, Stdev: 0.03290541745942384 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 288}\n",
      "Means: 0.7346399823824564, Stdev: 0.03361244620422075 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 320}\n",
      "Means: 0.7145599921544393, Stdev: 0.022703798896613805 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 352}\n",
      "Means: 0.7521199981371561, Stdev: 0.010896895068665235 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 384}\n",
      "Means: 0.7541733384132385, Stdev: 0.008264325745607681 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 416}\n",
      "Means: 0.7442800005276998, Stdev: 0.0092253750711876 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 448}\n",
      "Means: 0.7385999957720438, Stdev: 0.025256934236094316 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 480}\n",
      "Means: 0.7538266777992249, Stdev: 0.0011506868865487744 with: {'activation': 'softmax', 'learning_rate': 0.01, 'units': 512}\n",
      "Means: 0.7193199992179871, Stdev: 0.008353834910924427 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 32}\n",
      "Means: 0.6371733347574869, Stdev: 0.07497159141297588 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 64}\n",
      "Means: 0.6821866830190023, Stdev: 0.013356811827376483 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 96}\n",
      "Means: 0.6969466805458069, Stdev: 0.04833159981540848 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 128}\n",
      "Means: 0.6342533230781555, Stdev: 0.03741585845098002 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 160}\n",
      "Means: 0.6191466649373373, Stdev: 0.012787840919281987 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 192}\n",
      "Means: 0.6777599851290385, Stdev: 0.05078764103090503 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 224}\n",
      "Means: 0.6426933209101359, Stdev: 0.033998210821155214 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 256}\n",
      "Means: 0.6454400022824606, Stdev: 0.02973440699139806 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 288}\n",
      "Means: 0.6312400102615356, Stdev: 0.005645910473538399 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 320}\n",
      "Means: 0.6451466679573059, Stdev: 0.0301910631846413 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 352}\n",
      "Means: 0.648199995358785, Stdev: 0.08073212076987275 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 384}\n",
      "Means: 0.6381333271662394, Stdev: 0.002774615807713911 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 416}\n",
      "Means: 0.6325200001398722, Stdev: 0.005155288983703194 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 448}\n",
      "Means: 0.6512399911880493, Stdev: 0.025086593514665485 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 480}\n",
      "Means: 0.6470399896303812, Stdev: 0.02735591050953933 with: {'activation': 'softmax', 'learning_rate': 0.001, 'units': 512}\n"
     ]
    }
   ],
   "source": [
    "# save start time \n",
    "start = time()\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=hyper_parameters, \n",
    "                    n_jobs=-2, \n",
    "                    verbose=1, \n",
    "                    cv=3)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# save end time \n",
    "end = time()\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.96308587392171"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total run time \n",
    "total_run_time_in_miniutes = (end - start)/60\n",
    "total_run_time_in_miniutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu', 'learning_rate': 0.001, 'units': 512}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 5ms/step - loss: 0.4824 - accuracy: 0.8592\n"
     ]
    }
   ],
   "source": [
    "# because all other optimization approaches are reporting test set score\n",
    "# let's calculate the test set score in this case \n",
    "best_model = grid_result.best_estimator_\n",
    "test_acc = best_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8591600060462952"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Results\n",
    " \n",
    "Identify and write the the best performing hyperparamter combination and model score. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9577db883482c6cded3836e5cfbf5a74",
     "grade": true,
     "grade_id": "cell-eb06d682d2790f6e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "While the GridSearchCV gave the same hyperparameters, the conclusions took too long to perform. In addition, it did not allow for epoch training like Bayesian or RandomSearch did. Overall, for time and overall accuracy, I would choose either of the latter two before GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
    "\n",
    "Even if we did find a way to pass in the original test set into GridSearchCV, we can see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search when we consider the trade-offs of run time and locating the best performing model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Stretch Goals\n",
    "\n",
    "- Feel free to run whatever gridserach experiments on whatever models you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is your open playground - be free to explore as you wish "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_433_Tune_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
